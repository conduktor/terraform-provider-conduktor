---
page_title: "Conduktor : conduktor_console_topic_v2 "
subcategory: "kafka/v2"
description: |-
    Resource for managing Kafka topics with Conduktor Console.
    This resource allows you to create, read, update and delete kafka topics in Conduktor.
---

# conduktor_console_topic_v2

Resource for managing Kafka topics.
This resource allows you to create, read, update and delete kafka topics in Conduktor.

> [!NOTE]
> - It is essential to set `lifecycle { prevent_destroy = true }` on production instances to prevent accidental topic deletion and data loss.
> - This setting rejects plans that would destroy or recreate the topic, such as attempting to change uneditable attributes.
> - Read more about it in the [Terraform docs](https://www.terraform.io/language/meta-arguments/lifecycle#prevent_destroy).
> - Some providers may set default configs that will appear after the initial apply. In these cases resource definitions may need to be updated e.g. "cleanup.policy" = "delete" after creating a Redpanda topic

> [!WARNING]
> - This resource is officially supported from Conduktor Console `1.30.0` and newer.
> - Usage of this resource with older Console version might result in unexpected behavior.
> - e.g. `sql_storage` has been made available from Conduktor Console `1.30.0`.

## Example Usage

### Topic with prevent_destroy
```terraform
resource "conduktor_console_topic_v2" "production_topic" {
  name    = "production-topic"
  cluster = "kafka-cluster"
  spec = {
    partitions         = 10
    replication_factor = 1
  }

  lifecycle {
    prevent_destroy = true
  }
}
```

### Simple topic
```terraform
resource "conduktor_console_topic_v2" "simple" {
  name    = "simple"
  cluster = "kafka-cluster"
  labels = {
    domain = "clickstream"
  }
  description = "# Simple kafka topic"
  spec = {
    partitions         = 3
    replication_factor = 1
    configs = {
      "cleanup.policy" = "delete"
    }
  }
}
```

### Complex topic
```terraform
resource "conduktor_console_topic_v2" "complex" {
  name    = "complex"
  cluster = "kafka-cluster"
  labels = {
    domain  = "clickstream"
    appcode = "clk"
  }
  catalog_visibility      = "PRIVATE"
  description_is_editable = false
  description             = "# Complex kafka topic"
  sql_storage = {
    retention_time_in_second = 60000
    enabled                  = true
  }
  spec = {
    partitions         = 3
    replication_factor = 1
    configs = {
      "cleanup.policy" = "delete",
      "retention.ms"   = "60000"
    }
  }
}
```


<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `cluster` (String) Kafka cluster name linked with Kafka topic. Must already exist in Conduktor Console. Any change will require the Topic to be destroyed and re-created
- `name` (String) Topic name, must be unique, acts as an ID for import. Any change will require the Topic to be destroyed and re-created
- `spec` (Attributes) Topic specification (see [below for nested schema](#nestedatt--spec))

### Optional

- `catalog_visibility` (String) Catalog visibility for the topic, valid values are: PRIVATE, PUBLIC
- `description` (String) Topic description
- `description_is_editable` (Boolean) is optional (defaults 'true'). Defines whether the description can be updated in the UI
- `labels` (Map of String) Custom labels for the topic resource. Used in Conduktor's topic catalog and UI
- `sql_storage` (Attributes) (see [below for nested schema](#nestedatt--sql_storage))

<a id="nestedatt--spec"></a>
### Nested Schema for `spec`

Required:

- `partitions` (Number) Immutable field. Any change will require the Topic to be destroyed and re-created
- `replication_factor` (Number) Immutable field. Any change will require the Topic to be destroyed and re-created

Optional:

- `configs` (Map of String) Must be valid Kafka Topic configs


<a id="nestedatt--sql_storage"></a>
### Nested Schema for `sql_storage`

Required:

- `retention_time_in_second` (Number) When storing a topic's data for Conduktor SQL search, how long to retain the topic data in the database

Optional:

- `enabled` (Boolean) Whether to store topic data in the database, to enable Conduktor SQL search of a topic




## Import

In order to import a Kafka topics into Conduktor, you need to know the Kafka cluster ID and the Kafka Topic ID.

The import ID is constructed as follows: `< cluster_id >/< topic_id >`.

For example, using an [`import` block](https://developer.hashicorp.com/terraform/language/import) :
```terraform
import {
  to = conduktor_console_topic_v2.example
  id = "my-cluster/my-topic" # Import "my-topic" Topic for "my-cluster" Kafka cluster
}
```

Using the `terraform import` command:
```shell
terraform import conduktor_console_topic_v2.example my-cluster/my-topic
```
